{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Árboles de decisión y Random Forests"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Introducción teórica a los árboles"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Los métodos de machine learning basados en árboles comprenden un conjunto de técnicas de aprendizaje supervisado, que permiten segmentar el espacio de los predictores en regiones simples, dentro de las cuales es más sencillo manejar las interacciones y predecir la variable respuesta. Aunque su porcentaje mayoritario de aplicación es en el campo de la clasificación, también se pueden emplear en predicciones de variables continuas.\n",
    "\n",
    "**Ventajas:**\n",
    "\n",
    "- Facilidad de interpretación, no es necesario tener conocimientos estadísticos.\n",
    "\n",
    "- Se pueden graficar e interpretar de forma sencilla mientras que el modelo no tenga más de un árbol.\n",
    "\n",
    "- Se pueden introducir variables numéricas y categóricas, sin necesidad de dummificar éstas últimas. No obstante, a veces es recomdable categorizar las variables numéricas. \n",
    "\n",
    "- Al tratarse de métodos no paramétricos, no es necesario que se cumpla ningún tipo de distribución específica en los datos.\n",
    "\n",
    "- Requieren menos limpieza y trabajo de preprocesado que otros métodos.\n",
    "\n",
    "- Son robustos frente a la presencia de outliers.\n",
    "\n",
    "- Son muy útiles en la exploración de datos, permiten identificar de forma rápida y eficiente las variables más importantes.\n",
    "\n",
    "- Son capaces de seleccionar predictores de forma automática.\n",
    "\n",
    "**Desventajas:**\n",
    "\n",
    "- El árbol de decisión tiene tendencia al overfitting. Este inconveniente se puede mitigar combinando múltiples árboles y obteniendo valores promedio (bagging, random forests, boosting).\n",
    "\n",
    "- Debido a la categorización de las variables continuas, suelen obtener mejores resultados en ejercicios de clasificación que en los de regresión.\n",
    "\n",
    "- Tal y como se describe más adelante, la creación de las ramificaciones de los árboles se consigue mediante el algoritmo de recursive binary splitting. Este algoritmo identifica y evalúa las posibles divisiones de cada predictor acorde a una determinada medida (RSS, Gini, entropía…). Los predictores continuos o predictores cualitativos con muchos niveles tienen mayor probabilidad de contener, solo por azar, algún punto de corte óptimo, por lo que suelen verse favorecidos en la creación de los árboles."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1. Árboles de clasificación"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1.1. Idea intuitiva    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1.2. Construcción "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
